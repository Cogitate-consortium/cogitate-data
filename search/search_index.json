{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Cogitate Data Release Documentation Version Author(s) 1.0 Kahraman, K., Sripad, P., Brown, T., Melloni, L., Bonacchi, N. Date Editor(s) April, 2023 Kahraman, K., Sripad, P., Brown, T., Melloni, L., Bonacchi, N. Scraps Intro to Cogitate goal and experiment [link to website] \u2192 What is in the release Data release description \u2192 Subjects & Modalities \u2192 Format of the release (Database & archival format) \u2192 Raw / BIDS / sample_data for analysis repro \u2192 Link to Bundles Miscellaneous \u2192 SOPs \u2192 Anonymization Examples and Tutorials Processing Tools Matlab and Python code used to analyze or pre-process the data Any MRI pulse sequences (binaries) developed and used to acquire the data All (de-identified, PHI removed and defaced) MRI, neurophysiological, eye tracking and behavioral data adhering to the FAIR principle. The FAIR Data Principles refer to a set of guidelines to make data findable, accessible, interoperable and reusable (Wilkinson et al., 2016). Specifically, all data will be transformed to Brain Imaging Data Structure BIDS format ( http://bids.neuroimaging.io ) and all metadata will be made available. Data will be converted to shareable data formats (BIDS). The neurophysiological recordings (clinical ECoG and experimental electrodes), relevant task data, electrode coordinates in MNI space and essential, de-identified clinical data using NINDS Common Data Elements (age, sex, duration of epilepsy, epilepsy etiology, preoperative imaging findings) and schematics of seizure onset areas will also be made available. In order to access or download files containing behavioral, eye tracking, neurophysiological or MRI data, users will have to register. As they register, they will agree to restrictions against attempting to identify study participants, restrictions on redistribution of the data to third parties, and to properly acknowledge the data resource. [e] This document provides information and guidance on how to use data released by the TWCF COGITATE Project consortium. The COGITATE Project aimed to study \u2026 Given the richness of the COGITATE datasets and their utility for a wide range of research purposes, it is important that potential users understand what data are currently released, how the datasets are organized, how they can be accessed, what has changed since the last releases \u2026. (copied by HCP 1200 Subjects data release for orientation) Final COGITATE Study Overall Totals: X subjects with XY Modality COGITATE subjects include X This data release included behavioral measures, eye tracking, fMRI and M-EEG data on subjects collected in the COGITATE production data phase (YEAR-YEAR) including unprocessed (raw defaced), pre-processed and final processed fMRI etc. pp. fMRI data 50 Subjects from each lab Total: 122 Measurements? Techniques? Acquisition sites: Donders Center for Cognitive Neuroimaging (DCCN) Yale Magnetic Resonance Research Center (MRRC) SOP: Standard Operating Procedures data acqusition: Anatomical and functional MRI data were acquired on two 3T Prisma scanners (Siemens, Erlangen, Germany), using 32-channel head coils. Anatomical images were acquired using a T1-weighted magnetization prepared rapid gradient echo sequence (MP-RAGE; GRAPPA acceleration factor = 2, TR/TE = 2300/3.03 ms, voxel size 1 mm isotropic, 8\u00b0 flip angle). Functional images were acquired using a whole-brain T2-weighted multiband-4 sequence (time repetition [TR] / time echo [TE] = 1500/39.6 ms, 68 slices, voxel size 2 mm isotropic, 75\u00b0 flip angle, A/P phase encoding direction, FOV = 210 mm, BW = 2090 Hz/Px). In addition, a single band reference image was acquired before each run. To allow for signal stabilization, the first three volumes of each run were discarded. Additional scans using the same T2-weighted sequence, but with inverted phase encoding direction (inverted RO/PE polarity), were collected at multiple points throughout the experiments. data preprocessing: Source DICOM data was converted to a BIDS compliant dataset using BIDScoin v3.6.3 (https://bidscoin.readthedocs.io). This includes converting DICOM data to nifti using dcm2niix (Li et al., 2016, PMID: 26945974) and creating event files using custom Python (Python Software Foundation, RRID:SCR_008394) code. BIDS compliance of the resulting dataset was controlled using BIDS-Validator (https://doi.org/10.5281/zenodo.3688707). Subsequently, MRI data quality control was performed using MRIQC (Esteban et al., 2017) and custom scripts for data rejection. All (f)MRI data was preprocessed using fMRIPrep 20.2.3 (Esteban, Markiewicz, et al. (2018); Esteban, Blair, et al. (2018); RRID:SCR_016216), based on Nipype 1.6.1 (Gorgolewski et al. (2011); Gorgolewski et al. (2018); RRID:SCR_002502). In addition, analysis specific preprocessing was performed using FSL 6.0.2 (FMRIB Software Library; Oxford, UK; Smith et al., 2004, RRID:SCR_002823) and custom Python scripts using the following packages: NumPy XX (van der Walt et al., 2011, RRID:SCR_008633), Pandas XX (McKinney et al., 2010) NiBabel XX (https://doi.org/10.5281/zenodo.4295521), SciPy XX (Jones et al., 2001, RRID:SCR_008058), Matplotlib XX (Hunter, 2007, RRID:SCR_008624), Scikit-learn XX (Pedregosa et al., 2011, RRID:SCR_002577). COPY PASTED THE DOC \u201cfMRI_data_workflow_and_analyses\u201d FROM KEEPER/GOOGLE DRIVE fMRI data quality control: MRI data quality was screened using MRIQC (Esteban et al., 2017). Visual inspection of (f)MRI data was performed on the visual reports output by MRIQC and datasets with artifacts or other indicators of low data quality flagged by a trained observer, not involved in the MRI data acquisition. If the detected problems with data quality were judged severe enough to warrant potential exclusion, data was inspected by an additional observer. In total XX datasets were excluded due to low-quality. Quantitative data rejection. Additionally, fMRI datasets with excessive motion or subpar signal quality were rejected using the image quality metrics reported by MRIQC. The percentage of fMRI volumes that exceeded a thresholded of 0.2mm framewise displacement (fd_perc) was calculated per run and then averaged across runs per MRI session. Similarly, the DVARS (Power et al., 2012; dvars_nstd) was extracted per run and then averaged for each session. Finally, each MRI session that showed 2SD above the group mean on percentage framewise displacement or DVARS was rejected from all MRI data analysis. This resulted in an additional exclusion of XX MRI datasets. Anatomical data preprocessing: Per participant, the T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (Tustison et al. 2010), distributed with ANTs 2.3.3 (Avants et al. 2008, RRID:SCR_004757), and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (FSL 5.0.9, RRID:SCR_002823, Zhang, Brady, and Smith 2001). Brain surfaces were reconstructed using recon-all (FreeSurfer 6.0.1, RRID:SCR_001847, Dale, Fischl, and Sereno 1999), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle (RRID:SCR_002438, Klein et al. 2017). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.3.3), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: ICBM 152 Nonlinear Asymmetrical template version 2009c [Fonov et al. (2009), RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym], Functional data preprocessing: For each of the fMRI BOLD runs per subject, the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. A B0-nonuniformity map (or fieldmap) was estimated based on two (or more) echo-planar imaging (EPI) references with opposing phase-encoding directions, with 3dQwarp Cox and Hyde (1997) (AFNI 20160207). Based on the estimated susceptibility distortion, a corrected EPI (echo-planar imaging) reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (Greve and Fischl 2009). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9, Jenkinson et al. 2002). The BOLD time-series (without slice-timing correction) were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in MNI152NLin2009cAsym space. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and three region-wise global signals. FD was computed using two formulations following Power (absolute sum of relative motions, Power et al. (2014)) and Jenkinson (relative root mean square displacement between affines, Jenkinson et al. (2002)). FD and DVARS are calculated for each functional run, both using their implementations in Nipype (following the definitions by Power et al. 2014). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (CompCor, Behzadi et al. 2007). Principal components are estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128s cut-off) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 2% variable voxels within the brain mask. For aCompCor, three probabilistic masks (CSF, WM and combined CSF+WM) are generated in anatomical space. The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, the aCompCor masks are subtracted a mask of pixels that likely contain a volume fraction of GM. This mask is obtained by dilating a GM mask extracted from the FreeSurfer\u2019s aseg segmentation, and it ensures components are not extracted from voxels containing a minimal fraction of GM. Finally, these masks are resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components\u2019 time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (Satterthwaite et al. 2013). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardised DVARS were annotated as motion outliers. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (Lanczos 1964). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer). What is missing? What should go somewhere else? iEEG (ECoG) data 25 Subjects from each lab Total: ca. 26 (is this still right?) Measurements SOPs Techniques etc. Acquisition sites: Harvard University at Boston Children's Hospital Brigham and Women's Hospital New York University Langone (NYU) University of Wisconsin = LINK HERE INFORMATION FROM OTHER SLAB POSTS MEEG data 50 Subjects from each lab Total: 102 measurements SOPs Techniques etc. Acquisition sites: University of Birmingham's Centre for Human Brain Health (CHBH) Peking University (PKU) = LINK HERE INFORMATION FROM OTHER SLAB POSTS = 550 Datasets total \u2192 Eye tracking in all modalities (all sites have same eye tracker except NYU) NOT SURE HOW TO HANDLE INFORMATION/WHERE TO PUT \u2192 Behavior data should get a separate section SUMMARY SUBJECTS INFO [graphs?] Age Gender Dominant Hand Eye dominance Visual acuity (eye chart results, 20/20, 20/30, 20/40, etc.) No glasses, glasses, contact lenses Dioptre for both eyes for subject wearing glasses or contact lenses (if known) Colour blindness Auditory sensitivity Level of education Ethnicity Primary language Secondary language COGITATE Data Release arc-cogitate.com [a] MAybe add summary graphs for subjects demographics. Do we have these? [b] Marked as resolved [c] Re-opened [d] to each release type [e] Review this and put it in the intro/description","title":"Welcome to the Cogitate Data Release Documentation"},{"location":"#welcome-to-the-cogitate-data-release-documentation","text":"Version Author(s) 1.0 Kahraman, K., Sripad, P., Brown, T., Melloni, L., Bonacchi, N. Date Editor(s) April, 2023 Kahraman, K., Sripad, P., Brown, T., Melloni, L., Bonacchi, N. Scraps Intro to Cogitate goal and experiment [link to website] \u2192 What is in the release Data release description \u2192 Subjects & Modalities \u2192 Format of the release (Database & archival format) \u2192 Raw / BIDS / sample_data for analysis repro \u2192 Link to Bundles Miscellaneous \u2192 SOPs \u2192 Anonymization Examples and Tutorials Processing Tools Matlab and Python code used to analyze or pre-process the data Any MRI pulse sequences (binaries) developed and used to acquire the data All (de-identified, PHI removed and defaced) MRI, neurophysiological, eye tracking and behavioral data adhering to the FAIR principle. The FAIR Data Principles refer to a set of guidelines to make data findable, accessible, interoperable and reusable (Wilkinson et al., 2016). Specifically, all data will be transformed to Brain Imaging Data Structure BIDS format ( http://bids.neuroimaging.io ) and all metadata will be made available. Data will be converted to shareable data formats (BIDS). The neurophysiological recordings (clinical ECoG and experimental electrodes), relevant task data, electrode coordinates in MNI space and essential, de-identified clinical data using NINDS Common Data Elements (age, sex, duration of epilepsy, epilepsy etiology, preoperative imaging findings) and schematics of seizure onset areas will also be made available. In order to access or download files containing behavioral, eye tracking, neurophysiological or MRI data, users will have to register. As they register, they will agree to restrictions against attempting to identify study participants, restrictions on redistribution of the data to third parties, and to properly acknowledge the data resource. [e] This document provides information and guidance on how to use data released by the TWCF COGITATE Project consortium. The COGITATE Project aimed to study \u2026 Given the richness of the COGITATE datasets and their utility for a wide range of research purposes, it is important that potential users understand what data are currently released, how the datasets are organized, how they can be accessed, what has changed since the last releases \u2026. (copied by HCP 1200 Subjects data release for orientation) Final COGITATE Study Overall Totals: X subjects with XY Modality COGITATE subjects include X This data release included behavioral measures, eye tracking, fMRI and M-EEG data on subjects collected in the COGITATE production data phase (YEAR-YEAR) including unprocessed (raw defaced), pre-processed and final processed fMRI etc. pp. fMRI data 50 Subjects from each lab Total: 122 Measurements? Techniques? Acquisition sites: Donders Center for Cognitive Neuroimaging (DCCN) Yale Magnetic Resonance Research Center (MRRC) SOP: Standard Operating Procedures data acqusition: Anatomical and functional MRI data were acquired on two 3T Prisma scanners (Siemens, Erlangen, Germany), using 32-channel head coils. Anatomical images were acquired using a T1-weighted magnetization prepared rapid gradient echo sequence (MP-RAGE; GRAPPA acceleration factor = 2, TR/TE = 2300/3.03 ms, voxel size 1 mm isotropic, 8\u00b0 flip angle). Functional images were acquired using a whole-brain T2-weighted multiband-4 sequence (time repetition [TR] / time echo [TE] = 1500/39.6 ms, 68 slices, voxel size 2 mm isotropic, 75\u00b0 flip angle, A/P phase encoding direction, FOV = 210 mm, BW = 2090 Hz/Px). In addition, a single band reference image was acquired before each run. To allow for signal stabilization, the first three volumes of each run were discarded. Additional scans using the same T2-weighted sequence, but with inverted phase encoding direction (inverted RO/PE polarity), were collected at multiple points throughout the experiments. data preprocessing: Source DICOM data was converted to a BIDS compliant dataset using BIDScoin v3.6.3 (https://bidscoin.readthedocs.io). This includes converting DICOM data to nifti using dcm2niix (Li et al., 2016, PMID: 26945974) and creating event files using custom Python (Python Software Foundation, RRID:SCR_008394) code. BIDS compliance of the resulting dataset was controlled using BIDS-Validator (https://doi.org/10.5281/zenodo.3688707). Subsequently, MRI data quality control was performed using MRIQC (Esteban et al., 2017) and custom scripts for data rejection. All (f)MRI data was preprocessed using fMRIPrep 20.2.3 (Esteban, Markiewicz, et al. (2018); Esteban, Blair, et al. (2018); RRID:SCR_016216), based on Nipype 1.6.1 (Gorgolewski et al. (2011); Gorgolewski et al. (2018); RRID:SCR_002502). In addition, analysis specific preprocessing was performed using FSL 6.0.2 (FMRIB Software Library; Oxford, UK; Smith et al., 2004, RRID:SCR_002823) and custom Python scripts using the following packages: NumPy XX (van der Walt et al., 2011, RRID:SCR_008633), Pandas XX (McKinney et al., 2010) NiBabel XX (https://doi.org/10.5281/zenodo.4295521), SciPy XX (Jones et al., 2001, RRID:SCR_008058), Matplotlib XX (Hunter, 2007, RRID:SCR_008624), Scikit-learn XX (Pedregosa et al., 2011, RRID:SCR_002577). COPY PASTED THE DOC \u201cfMRI_data_workflow_and_analyses\u201d FROM KEEPER/GOOGLE DRIVE fMRI data quality control: MRI data quality was screened using MRIQC (Esteban et al., 2017). Visual inspection of (f)MRI data was performed on the visual reports output by MRIQC and datasets with artifacts or other indicators of low data quality flagged by a trained observer, not involved in the MRI data acquisition. If the detected problems with data quality were judged severe enough to warrant potential exclusion, data was inspected by an additional observer. In total XX datasets were excluded due to low-quality. Quantitative data rejection. Additionally, fMRI datasets with excessive motion or subpar signal quality were rejected using the image quality metrics reported by MRIQC. The percentage of fMRI volumes that exceeded a thresholded of 0.2mm framewise displacement (fd_perc) was calculated per run and then averaged across runs per MRI session. Similarly, the DVARS (Power et al., 2012; dvars_nstd) was extracted per run and then averaged for each session. Finally, each MRI session that showed 2SD above the group mean on percentage framewise displacement or DVARS was rejected from all MRI data analysis. This resulted in an additional exclusion of XX MRI datasets. Anatomical data preprocessing: Per participant, the T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (Tustison et al. 2010), distributed with ANTs 2.3.3 (Avants et al. 2008, RRID:SCR_004757), and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (FSL 5.0.9, RRID:SCR_002823, Zhang, Brady, and Smith 2001). Brain surfaces were reconstructed using recon-all (FreeSurfer 6.0.1, RRID:SCR_001847, Dale, Fischl, and Sereno 1999), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle (RRID:SCR_002438, Klein et al. 2017). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.3.3), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: ICBM 152 Nonlinear Asymmetrical template version 2009c [Fonov et al. (2009), RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym], Functional data preprocessing: For each of the fMRI BOLD runs per subject, the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. A B0-nonuniformity map (or fieldmap) was estimated based on two (or more) echo-planar imaging (EPI) references with opposing phase-encoding directions, with 3dQwarp Cox and Hyde (1997) (AFNI 20160207). Based on the estimated susceptibility distortion, a corrected EPI (echo-planar imaging) reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w reference using bbregister (FreeSurfer) which implements boundary-based registration (Greve and Fischl 2009). Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9, Jenkinson et al. 2002). The BOLD time-series (without slice-timing correction) were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in MNI152NLin2009cAsym space. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and three region-wise global signals. FD was computed using two formulations following Power (absolute sum of relative motions, Power et al. (2014)) and Jenkinson (relative root mean square displacement between affines, Jenkinson et al. (2002)). FD and DVARS are calculated for each functional run, both using their implementations in Nipype (following the definitions by Power et al. 2014). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (CompCor, Behzadi et al. 2007). Principal components are estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128s cut-off) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 2% variable voxels within the brain mask. For aCompCor, three probabilistic masks (CSF, WM and combined CSF+WM) are generated in anatomical space. The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, the aCompCor masks are subtracted a mask of pixels that likely contain a volume fraction of GM. This mask is obtained by dilating a GM mask extracted from the FreeSurfer\u2019s aseg segmentation, and it ensures components are not extracted from voxels containing a minimal fraction of GM. Finally, these masks are resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components\u2019 time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (Satterthwaite et al. 2013). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardised DVARS were annotated as motion outliers. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (Lanczos 1964). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer). What is missing? What should go somewhere else? iEEG (ECoG) data 25 Subjects from each lab Total: ca. 26 (is this still right?) Measurements SOPs Techniques etc. Acquisition sites: Harvard University at Boston Children's Hospital Brigham and Women's Hospital New York University Langone (NYU) University of Wisconsin = LINK HERE INFORMATION FROM OTHER SLAB POSTS MEEG data 50 Subjects from each lab Total: 102 measurements SOPs Techniques etc. Acquisition sites: University of Birmingham's Centre for Human Brain Health (CHBH) Peking University (PKU) = LINK HERE INFORMATION FROM OTHER SLAB POSTS = 550 Datasets total \u2192 Eye tracking in all modalities (all sites have same eye tracker except NYU) NOT SURE HOW TO HANDLE INFORMATION/WHERE TO PUT \u2192 Behavior data should get a separate section SUMMARY SUBJECTS INFO [graphs?] Age Gender Dominant Hand Eye dominance Visual acuity (eye chart results, 20/20, 20/30, 20/40, etc.) No glasses, glasses, contact lenses Dioptre for both eyes for subject wearing glasses or contact lenses (if known) Colour blindness Auditory sensitivity Level of education Ethnicity Primary language Secondary language COGITATE Data Release arc-cogitate.com [a] MAybe add summary graphs for subjects demographics. Do we have these? [b] Marked as resolved [c] Re-opened [d] to each release type [e] Review this and put it in the intro/description","title":"Welcome to the Cogitate Data Release Documentation"},{"location":"01_intro/","text":"Introduction COGITATE is a groundbreaking endeavor within Open Science, featuring a pre-registered adversarial collaboration aimed at resolving the debate between two prominent theories of consciousness: Integrated Information Theory (IIT) and Global Neuronal Workspace theory (GNW). This initiative encompasses eleven distinct research institutions spanning three continents, utilizing three distinct neuroimaging methods in human neuroscience\u2014fMRI, M-EEG, and iEEG\u2014across an extensive cohort of human volunteers and patients. A comprehensive outline of Experiment 1 is available in our pre-registration document , preprint manuscript , and forthcoming publication. TODO: - CODE Release [**[link to MSP-1 repo](https://github.com/Cogitate-consortium/cogitate-msp1)**] explain here, if too big add chapter at end Access COGITATE data The primary objective of this data release is to provide a comprehensive dataset that will enable further research in the field of consciousness studies. By making our data publicly available, we aim to foster transparency, encourage reproducibility, and facilitate collaborative efforts in unraveling the complexities of human consciousness. This data release encompasses a wide array of data types, including but not limited to: Behavioral Data Eye Tracking Data Neurophysiological Data MRI Data For a detailed description of each data type and its corresponding file formats, please refer to the \"Data Description\" and \"File Type Glossary\" sections. The dataset is intended for use by researchers, clinicians, and data scientists interested in the study of consciousness. Users are required to register to access or download data. By registering, users agree to restrictions against attempting to identify study participants and redistributing the data to third parties. Proper acknowledgment of the data resource is also mandatory. We would like to extend our gratitude to all team members and participants who have made this study and data release possible.","title":"Introduction"},{"location":"01_intro/#introduction","text":"COGITATE is a groundbreaking endeavor within Open Science, featuring a pre-registered adversarial collaboration aimed at resolving the debate between two prominent theories of consciousness: Integrated Information Theory (IIT) and Global Neuronal Workspace theory (GNW). This initiative encompasses eleven distinct research institutions spanning three continents, utilizing three distinct neuroimaging methods in human neuroscience\u2014fMRI, M-EEG, and iEEG\u2014across an extensive cohort of human volunteers and patients. A comprehensive outline of Experiment 1 is available in our pre-registration document , preprint manuscript , and forthcoming publication. TODO: - CODE Release [**[link to MSP-1 repo](https://github.com/Cogitate-consortium/cogitate-msp1)**] explain here, if too big add chapter at end","title":"Introduction"},{"location":"01_intro/#access-cogitate-data","text":"The primary objective of this data release is to provide a comprehensive dataset that will enable further research in the field of consciousness studies. By making our data publicly available, we aim to foster transparency, encourage reproducibility, and facilitate collaborative efforts in unraveling the complexities of human consciousness. This data release encompasses a wide array of data types, including but not limited to: Behavioral Data Eye Tracking Data Neurophysiological Data MRI Data For a detailed description of each data type and its corresponding file formats, please refer to the \"Data Description\" and \"File Type Glossary\" sections. The dataset is intended for use by researchers, clinicians, and data scientists interested in the study of consciousness. Users are required to register to access or download data. By registering, users agree to restrictions against attempting to identify study participants and redistributing the data to third parties. Proper acknowledgment of the data resource is also mandatory. We would like to extend our gratitude to all team members and participants who have made this study and data release possible.","title":"Access COGITATE data"},{"location":"02_subjects/","text":"Subjects The Cogitate dataset is a comprehensive collection of neuroimaging data, encompassing a total of 256 subjects. The majority of these participants are right-handed (229 out of 256), with a median age of 23 years. To ensure the integrity of the study, all investigations were conducted by impartial research teams, devoid of any theoretical bias, thereby mitigating the risk of confirmation bias. The dataset is unique in its multi-modal approach, employing three distinct neuroimaging techniques: functional magnetic resonance imaging (fMRI), magnetoencephalography (M-EEG), and intracranial electroencephalography (iEEG). Specifically, the fMRI modality included 120 healthy volunteers, all of whom were above the age of 18 and predominantly right-handed. These participants had no known history of psychiatric or neurological disorders and were recruited from the Yale Magnetic Resonance Research Center and the Donders Centre for Cognitive Neuroimaging. Similarly, the M-EEG modality comprised 102 healthy subjects, also above the age of 18, with no known psychiatric or neurological issues. These participants were sourced from the Centre for Human Brain Health at the University of Birmingham and the Center for MRI Research of Peking University. In contrast, the iEEG modality involved a more specialized cohort of 34 patients suffering from pharmaco-resistant focal epilepsy. These participants ranged in age from 10 to 65 years, had an IQ above 70, and met specific health criteria. They were recruited from multiple medical centers specializing in epilepsy treatment, such as the Comprehensive Epilepsy Center at New York University, Brigham and Women\u2019s Hospital, Boston Children\u2019s Hospital, and the University of Wisconsin School of Medicine and Public Health. For a more in-depth understanding of the data acquisition methods and protocols, we invite you to consult our Acquisition Modalities section. >>>>> gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> * Still no message from alex regarding the control experiment. TODO: * ~~Describe the subjects, all + by modality, e.g. iEEG subjects were epilepsy patients that underwent surgery for\u2026 Use plots of distributions and link to a clean version of the Table of subjects (linked below)~~ * Mention the subjects that were used for the pilot experiments, e.g. **N** additional subjects from, \u2026 were part of the initial control and pilot experiments during the development of the task (**ask Alex)** * Table of subjects here ([https://keeper.mpdl.mpg.de/lib/ea2a7955-5171-47b6-8195-5d35ca885bed/file/subjects_demographics.xlsx](https://keeper.mpdl.mpg.de/lib/ea2a7955-5171-47b6-8195-5d35ca885bed/file/subjects_demographics.xlsx)) migrate do doc folder","title":"Subjects"},{"location":"02_subjects/#subjects","text":"The Cogitate dataset is a comprehensive collection of neuroimaging data, encompassing a total of 256 subjects. The majority of these participants are right-handed (229 out of 256), with a median age of 23 years. To ensure the integrity of the study, all investigations were conducted by impartial research teams, devoid of any theoretical bias, thereby mitigating the risk of confirmation bias. The dataset is unique in its multi-modal approach, employing three distinct neuroimaging techniques: functional magnetic resonance imaging (fMRI), magnetoencephalography (M-EEG), and intracranial electroencephalography (iEEG). Specifically, the fMRI modality included 120 healthy volunteers, all of whom were above the age of 18 and predominantly right-handed. These participants had no known history of psychiatric or neurological disorders and were recruited from the Yale Magnetic Resonance Research Center and the Donders Centre for Cognitive Neuroimaging. Similarly, the M-EEG modality comprised 102 healthy subjects, also above the age of 18, with no known psychiatric or neurological issues. These participants were sourced from the Centre for Human Brain Health at the University of Birmingham and the Center for MRI Research of Peking University. In contrast, the iEEG modality involved a more specialized cohort of 34 patients suffering from pharmaco-resistant focal epilepsy. These participants ranged in age from 10 to 65 years, had an IQ above 70, and met specific health criteria. They were recruited from multiple medical centers specializing in epilepsy treatment, such as the Comprehensive Epilepsy Center at New York University, Brigham and Women\u2019s Hospital, Boston Children\u2019s Hospital, and the University of Wisconsin School of Medicine and Public Health. For a more in-depth understanding of the data acquisition methods and protocols, we invite you to consult our Acquisition Modalities section. >>>>> gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> >>>>> gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. ( Back to top )( Next alert ) >>>>> * Still no message from alex regarding the control experiment. TODO: * ~~Describe the subjects, all + by modality, e.g. iEEG subjects were epilepsy patients that underwent surgery for\u2026 Use plots of distributions and link to a clean version of the Table of subjects (linked below)~~ * Mention the subjects that were used for the pilot experiments, e.g. **N** additional subjects from, \u2026 were part of the initial control and pilot experiments during the development of the task (**ask Alex)** * Table of subjects here ([https://keeper.mpdl.mpg.de/lib/ea2a7955-5171-47b6-8195-5d35ca885bed/file/subjects_demographics.xlsx](https://keeper.mpdl.mpg.de/lib/ea2a7955-5171-47b6-8195-5d35ca885bed/file/subjects_demographics.xlsx)) migrate do doc folder","title":"Subjects"},{"location":"03_experiments/","text":"Experiments The Cogitate consortium performed two experiments: Exp1 & Exp2. The goals of these experiments were \u2026. Experiment 1: Neural Activity and Conscious Perception The primary aim of Experiment 1 is to investigate neural activity in response to consciously perceived stimuli. Two key factors were manipulated: the relevance of the stimulus to the task and the duration of stimulus perception. This design allows for the testing of several hypotheses, including the separation of consciousness-related activations from task-related activations and the identification of brain regions that convey information about the content of consciousness. The experiment follows a 3x3x4x2 factorial design, covering factors such as stimulus relevance, duration, category, and orientation. Sample sizes were determined based on common practices, resulting in 122 subjects for fMRI, 102 for M-EEG, and 34 for iEEG. All subjects met specific criteria, including age and health conditions, to ensure data quality. Protocol changes due to different acquisition modalities have been documented. The task code and stimuli are available in our repositories for further examination. ~~## Experiment 2: Video Game Engagement and Neural Activity~~ Objectives and Design The primary aim of Experiment 1 is to investigate neural activity in response to stimuli that are consciously perceived. This experiment is designed to manipulate two key factors: Relevance of the Stimulus to the Task : This factor is categorized into three levels\u2014Task-relevant target, Task-relevant non-target, and Task-irrelevant stimulus. Stimulus Duration : The stimuli are presented for durations of 500ms, 1000ms, and 1500ms. This design framework allows us to test several key hypotheses, including: Disentangling consciousness-related activations from task-related activations. Identifying brain regions that convey information about the content of consciousness. Examining the persistence of the content of consciousness over time. The experiment aims to test competing predictions of the Global Neuronal Workspace Theory (GNW) and Integrated Information Theory (IIT). A secondary objective is to identify potential neural correlates of consciousness (NCCs). Factorial Design The experiment follows a 3x3x4x2 factorial design, with the following factors: Relevance of Stimulus to the Task : Task-relevant target / Task-relevant non-target / Task-irrelevant stimulus. Stimulus Duration : 500ms / 1000ms / 1500ms. Stimulus Category : Faces / Objects / Letters / False-fonts. Stimulus Orientation : Side view / Front view. Sample Size The sample sizes were determined based on common practices in the literature, resulting in a total of 122 subjects for fMRI, 102 for M-EEG, and 34 for iEEG. All subjects met specific criteria, including age and health conditions, to ensure data quality. Task Description and Protocol Changes The task involves a series of visual stimuli presented to the subjects, as detailed in the pre-registration figures. Due to different acquisition modalities, some changes in the protocol were necessary. These changes are documented and can be compared with the original protocol. Code and Stimuli Repositories The task code for all modalities is available in a single repository. The stimuli used in the experiment can be accessed from the stimuli folder. TODO: * Intro: The Experiments performed by the Cogitate consortium were Exp1 & Exp 2. * The goals were\u2026 * What's in this release: Only Experiment 1 all modalities (see Acquisition Modalities section). More info in the methods section here -> link to publication * Explain task w/ figures from pre registration * Describe changes of protocol because of acquisition modality * Link to the task code repository [one repo for all modalities] * Link to the stimuli folder * End by mentioning that data for Experiment 2 (video game) will be part of a follow up release\u2026.","title":"Experiments"},{"location":"03_experiments/#experiments","text":"The Cogitate consortium performed two experiments: Exp1 & Exp2. The goals of these experiments were \u2026.","title":"Experiments"},{"location":"03_experiments/#experiment-1-neural-activity-and-conscious-perception","text":"The primary aim of Experiment 1 is to investigate neural activity in response to consciously perceived stimuli. Two key factors were manipulated: the relevance of the stimulus to the task and the duration of stimulus perception. This design allows for the testing of several hypotheses, including the separation of consciousness-related activations from task-related activations and the identification of brain regions that convey information about the content of consciousness. The experiment follows a 3x3x4x2 factorial design, covering factors such as stimulus relevance, duration, category, and orientation. Sample sizes were determined based on common practices, resulting in 122 subjects for fMRI, 102 for M-EEG, and 34 for iEEG. All subjects met specific criteria, including age and health conditions, to ensure data quality. Protocol changes due to different acquisition modalities have been documented. The task code and stimuli are available in our repositories for further examination. ~~## Experiment 2: Video Game Engagement and Neural Activity~~","title":"Experiment 1: Neural Activity and Conscious Perception"},{"location":"03_experiments/#objectives-and-design","text":"The primary aim of Experiment 1 is to investigate neural activity in response to stimuli that are consciously perceived. This experiment is designed to manipulate two key factors: Relevance of the Stimulus to the Task : This factor is categorized into three levels\u2014Task-relevant target, Task-relevant non-target, and Task-irrelevant stimulus. Stimulus Duration : The stimuli are presented for durations of 500ms, 1000ms, and 1500ms. This design framework allows us to test several key hypotheses, including: Disentangling consciousness-related activations from task-related activations. Identifying brain regions that convey information about the content of consciousness. Examining the persistence of the content of consciousness over time. The experiment aims to test competing predictions of the Global Neuronal Workspace Theory (GNW) and Integrated Information Theory (IIT). A secondary objective is to identify potential neural correlates of consciousness (NCCs).","title":"Objectives and Design"},{"location":"03_experiments/#factorial-design","text":"The experiment follows a 3x3x4x2 factorial design, with the following factors: Relevance of Stimulus to the Task : Task-relevant target / Task-relevant non-target / Task-irrelevant stimulus. Stimulus Duration : 500ms / 1000ms / 1500ms. Stimulus Category : Faces / Objects / Letters / False-fonts. Stimulus Orientation : Side view / Front view.","title":"Factorial Design"},{"location":"03_experiments/#sample-size","text":"The sample sizes were determined based on common practices in the literature, resulting in a total of 122 subjects for fMRI, 102 for M-EEG, and 34 for iEEG. All subjects met specific criteria, including age and health conditions, to ensure data quality.","title":"Sample Size"},{"location":"03_experiments/#task-description-and-protocol-changes","text":"The task involves a series of visual stimuli presented to the subjects, as detailed in the pre-registration figures. Due to different acquisition modalities, some changes in the protocol were necessary. These changes are documented and can be compared with the original protocol.","title":"Task Description and Protocol Changes"},{"location":"03_experiments/#code-and-stimuli-repositories","text":"The task code for all modalities is available in a single repository. The stimuli used in the experiment can be accessed from the stimuli folder. TODO: * Intro: The Experiments performed by the Cogitate consortium were Exp1 & Exp 2. * The goals were\u2026 * What's in this release: Only Experiment 1 all modalities (see Acquisition Modalities section). More info in the methods section here -> link to publication * Explain task w/ figures from pre registration * Describe changes of protocol because of acquisition modality * Link to the task code repository [one repo for all modalities] * Link to the stimuli folder * End by mentioning that data for Experiment 2 (video game) will be part of a follow up release\u2026.","title":"Code and Stimuli Repositories"},{"location":"04_data/","text":"Data description Data acquisition Although our data collection had a specific purpose, the data we gathered holds potential value for a range of diverse inquiries. Consequently, the COGITATE consortium has chosen to openly share all raw data collected, with the aim of facilitating its utilization for various research endeavors and promoting data reusability. This approach reflects our commitment to contributing to the broader scientific community. We have made available two primary formats for the data acquired during the experimental phase of the COGITATE project, specifically from Experiment 1: 1. Unprocessed Data: The unprocessed data format closely resembles the original acquired data, having undergone minimal processing to ensure compliance with GDPR/HIPAA anonymity standards. 2. BIDS Format (Brain Imaging Data Structure): BIDS format, widely adopted in cognitive neuroscience, enhances data reusability. To facilitate others in leveraging our data, we have released it in BIDS format. The conversion scripts employed to transform the raw data into BIDS format can be accessed via this link: [LINK]. We emphasize our explicit intent to share the entirety of our data, empowering the community to integrate it into their research. When utilizing the data, it is important to consider the contextual factors and limitations that may affect its interpretation. Release format and Modality Table: Release format -> ___ Modality Raw / Unprocessed Bids converted Bids derivatives / Pre-processed Additionally processed Eye Tracking & Behavioral data EDF EDF ASC ASC fMRI DICOM DICOM NIFTI NIFTI MEEG FIF FIF NPY NPY iEEG/ECoG EDF EDF EDF EDF File type glossary EDF files EDF files, also known as European Data Format files, are a standardized file format used for storing and exchanging time-series biological and physiological data. EDF files are commonly used in medical and scientific research contexts to record and analyze data from various types of measurements, such as electroencephalography (EEG), electrocardiography (ECG), electromyography (EMG), and other bioelectrical signals. EDF files are designed to accommodate data from multiple channels, allowing researchers to store and manage data collected simultaneously from different sensors or electrodes. The format supports both raw signal data and associated metadata, including information about sampling rates, units of measurement, patient demographics, and recording conditions. One of the key advantages of the EDF format is its portability and compatibility across different software and platforms. Many data analysis and visualization tools in the field of neuroscience and medicine support the EDF format, enabling researchers to share and collaborate on data without compatibility issues. It's worth noting that there are variations and extensions of the EDF format, such as EDF+ and EDF/EDF+ Converter, which provide additional features and improvements for specific applications. Overall, EDF files play an essential role in the storage and analysis of physiological and biological data in scientific and medical research. EDF files (Eye-tracking) EDF (EyeLink Data Format) is a file format used to store eye-tracking data collected during experiments using the EyeLink system. It plays a crucial role in the analysis of visual attention and gaze behavior in various research fields like psychology, cognitive science, human-computer interaction, to name a few. EDF files store data collected from eye-tracking experiments, which typically involve recording where a participant's eyes are fixating and how they move while viewing a stimulus, such as images or text. The EyeLink system, developed by SR Research Ltd., is a widely used eye-tracking hardware and software solution. The EyeLink system records various types of data during an experiment, including the position of the participant's gaze (where they are looking on the screen) and the timing of their eye movements. This data is then saved in EDF files, which have a specific structure that allows the analysis and interpretation of eye-tracking data using specialized software. EDF files contain information such as timestamps, gaze coordinates, pupil size, and information about the visual stimuli being presented. These files can be used to analyze patterns of visual attention, fixations (periods when the eyes are relatively stationary), saccades (rapid eye movements between fixations), and other eye movement behaviors. To work with EDF files, typically software is provided by SR Research or other third-party tools that can read and interpret the data in these files. This software allows researchers to visualize eye movement patterns, conduct statistical analyses, and draw conclusions about how participants process visual information. FIF files FIF files in MEEG (Magnetoencephalography and Electroencephalography) refer to the File Format for the Input and Output of MEG and EEG data. MEG and EEG are neuroimaging techniques used to study brain activity. FIF files are a standardized format developed by the MNE (Magnetoencephalography and Electroencephalography) community to store and exchange MEG and EEG data. FIF files contain various types of information related to neuroimaging data, including: Raw sensor data: MEG and EEG measurements recorded from sensors placed on the scalp or near the head. Event information: Time-stamped triggers or markers indicating the timing of events, such as stimulus presentations or subject responses. Sensor locations and orientations: Information about the physical positions and orientations of sensors used in the measurements. Head geometry: Information about the shape and structure of the subject's head, which is crucial for accurate source localization. Covariance matrices: Statistical information about the relationships between sensor measurements at different time points or frequencies. Anatomical MRI data: High-resolution structural images of the subject's brain, used for source localization and spatial alignment. The FIF file format ensures compatibility and interoperability between different MEG and EEG software tools and facilitates data sharing and collaboration in the neuroimaging research community. It's worth noting that MEG and EEG data can be quite complex and multidimensional, and the FIF format provides a structured and standardized way to store and manage this data. ASC files ASC files in the context of eye tracking refer to data files generated by eye tracking systems, which are used to record and analyze eye movement and gaze behavior. These files contain valuable information about a person's visual attention, including where they are looking, how long they are looking at certain areas, and how their gaze moves across a screen or a visual stimulus. ASC files typically store a time-stamped sequence of gaze data points, which include information such as: Timestamps: The exact time at which each gaze data point was recorded. Gaze Coordinates: The x and y coordinates on the screen where the person's gaze is directed. Pupil Diameter: The size of the person's pupil, which can provide insights into changes in visual processing or cognitive load. Fixations: Periods of stable gaze where the person is looking at a specific point without significant movement. Saccades: Rapid eye movements between fixations, indicating shifts in attention. Blinks: Instances when the person's eyes are closed, which can be important for data cleaning and analysis. Researchers and usability professionals use ASC files to study various aspects of human visual attention and cognitive processes. Eye tracking studies can provide insights into user behavior when interacting with websites, advertisements, software interfaces, or other visual stimuli. By analyzing ASC files, researchers can better understand how people engage with visual information, make decisions, and process visual content. Software tools and programming libraries are often used to process and analyze ASC files, extracting meaningful patterns and insights from the raw gaze data. These insights can be used to optimize user interfaces, design effective visual communication, and improve user experiences in various applications. DICOM files DICOM (Digital Imaging and Communications in Medicine) files are a standard format used for storing, transmitting, and managing medical images and related information. They are widely used in the field of radiology and other medical imaging specialties, such as MRI (Magnetic Resonance Imaging), CT (Computed Tomography), ultrasound, and more. DICOM files are designed to ensure interoperability and compatibility among different imaging devices, picture archiving and communication systems (PACS), and other healthcare information systems. These files contain not only the actual image data but also metadata and contextual information, such as patient information, image acquisition parameters, study details, and more. This comprehensive set of information allows medical professionals to accurately interpret and diagnose medical images, track patient history, and collaborate effectively across different healthcare facilities. DICOM files typically have a \".dcm\" file extension and adhere to a specific structure and data format defined by the DICOM standard. This standardization facilitates seamless exchange of medical images and data between different software and hardware platforms, contributing to improved patient care and medical research. NIFTI files NIFTI (Neuroimaging Informatics Technology Initiative) files are a widely used file format in the field of neuroimaging, particularly in the context of functional and structural magnetic resonance imaging (MRI) data. These files store three-dimensional brain images and related metadata, allowing researchers and clinicians to store, share, and analyze neuroimaging data. NIFTI files typically have the \".nii\" or \".nii.gz\" file extensions. The \".nii\" format is uncompressed, while the \".nii.gz\" format is compressed using gzip compression. These files contain information about the image dimensions, voxel sizes, data type (e.g., integer or floating-point values), and other relevant information. NIFTI was developed as an improvement over the earlier Analyze file format. It addresses certain limitations of the Analyze format, such as the ability to handle 3D and 4D data (3D data over time), improved support for different data types, and better compatibility with modern software tools. Researchers and medical professionals use NIFTI files to store various types of neuroimaging data, including structural MRI scans, functional MRI scans, and diffusion tensor imaging (DTI) data (which captures white matter tracts and connectivity patterns within the brain). NIFTI files are supported by many neuroimaging software packages, making them a crucial part of the neuroimaging data analysis workflow. Researchers can use these files for tasks such as preprocessing, visualization, statistical analysis, and creating anatomical atlases. NPY files NPY files, short for \"NumPy files,\" are a file format used in the Python programming language for storing and exchanging numerical data. NumPy is a popular library in Python that provides support for multi-dimensional arrays and matrices, along with various mathematical functions to operate on these arrays. NPY files are specifically used to save and load arrays and data structures created using NumPy. These files have the extension \".npy\" and are binary files that efficiently store the data in a format that preserves the array's shape, data type, and other metadata. This makes them suitable for fast and efficient storage and retrieval of large numerical datasets, which is especially useful in scientific computing, data analysis, and machine learning applications.","title":"Data description"},{"location":"04_data/#data-description","text":"","title":"Data description"},{"location":"04_data/#data-acquisition","text":"Although our data collection had a specific purpose, the data we gathered holds potential value for a range of diverse inquiries. Consequently, the COGITATE consortium has chosen to openly share all raw data collected, with the aim of facilitating its utilization for various research endeavors and promoting data reusability. This approach reflects our commitment to contributing to the broader scientific community. We have made available two primary formats for the data acquired during the experimental phase of the COGITATE project, specifically from Experiment 1: 1. Unprocessed Data: The unprocessed data format closely resembles the original acquired data, having undergone minimal processing to ensure compliance with GDPR/HIPAA anonymity standards. 2. BIDS Format (Brain Imaging Data Structure): BIDS format, widely adopted in cognitive neuroscience, enhances data reusability. To facilitate others in leveraging our data, we have released it in BIDS format. The conversion scripts employed to transform the raw data into BIDS format can be accessed via this link: [LINK]. We emphasize our explicit intent to share the entirety of our data, empowering the community to integrate it into their research. When utilizing the data, it is important to consider the contextual factors and limitations that may affect its interpretation. Release format and Modality Table: Release format -> ___ Modality Raw / Unprocessed Bids converted Bids derivatives / Pre-processed Additionally processed Eye Tracking & Behavioral data EDF EDF ASC ASC fMRI DICOM DICOM NIFTI NIFTI MEEG FIF FIF NPY NPY iEEG/ECoG EDF EDF EDF EDF","title":"Data acquisition"},{"location":"04_data/#file-type-glossary","text":"","title":"File type glossary"},{"location":"04_data/#edf-files","text":"EDF files, also known as European Data Format files, are a standardized file format used for storing and exchanging time-series biological and physiological data. EDF files are commonly used in medical and scientific research contexts to record and analyze data from various types of measurements, such as electroencephalography (EEG), electrocardiography (ECG), electromyography (EMG), and other bioelectrical signals. EDF files are designed to accommodate data from multiple channels, allowing researchers to store and manage data collected simultaneously from different sensors or electrodes. The format supports both raw signal data and associated metadata, including information about sampling rates, units of measurement, patient demographics, and recording conditions. One of the key advantages of the EDF format is its portability and compatibility across different software and platforms. Many data analysis and visualization tools in the field of neuroscience and medicine support the EDF format, enabling researchers to share and collaborate on data without compatibility issues. It's worth noting that there are variations and extensions of the EDF format, such as EDF+ and EDF/EDF+ Converter, which provide additional features and improvements for specific applications. Overall, EDF files play an essential role in the storage and analysis of physiological and biological data in scientific and medical research.","title":"EDF files"},{"location":"04_data/#edf-files-eye-tracking","text":"EDF (EyeLink Data Format) is a file format used to store eye-tracking data collected during experiments using the EyeLink system. It plays a crucial role in the analysis of visual attention and gaze behavior in various research fields like psychology, cognitive science, human-computer interaction, to name a few. EDF files store data collected from eye-tracking experiments, which typically involve recording where a participant's eyes are fixating and how they move while viewing a stimulus, such as images or text. The EyeLink system, developed by SR Research Ltd., is a widely used eye-tracking hardware and software solution. The EyeLink system records various types of data during an experiment, including the position of the participant's gaze (where they are looking on the screen) and the timing of their eye movements. This data is then saved in EDF files, which have a specific structure that allows the analysis and interpretation of eye-tracking data using specialized software. EDF files contain information such as timestamps, gaze coordinates, pupil size, and information about the visual stimuli being presented. These files can be used to analyze patterns of visual attention, fixations (periods when the eyes are relatively stationary), saccades (rapid eye movements between fixations), and other eye movement behaviors. To work with EDF files, typically software is provided by SR Research or other third-party tools that can read and interpret the data in these files. This software allows researchers to visualize eye movement patterns, conduct statistical analyses, and draw conclusions about how participants process visual information.","title":"EDF files (Eye-tracking)"},{"location":"04_data/#fif-files","text":"FIF files in MEEG (Magnetoencephalography and Electroencephalography) refer to the File Format for the Input and Output of MEG and EEG data. MEG and EEG are neuroimaging techniques used to study brain activity. FIF files are a standardized format developed by the MNE (Magnetoencephalography and Electroencephalography) community to store and exchange MEG and EEG data. FIF files contain various types of information related to neuroimaging data, including: Raw sensor data: MEG and EEG measurements recorded from sensors placed on the scalp or near the head. Event information: Time-stamped triggers or markers indicating the timing of events, such as stimulus presentations or subject responses. Sensor locations and orientations: Information about the physical positions and orientations of sensors used in the measurements. Head geometry: Information about the shape and structure of the subject's head, which is crucial for accurate source localization. Covariance matrices: Statistical information about the relationships between sensor measurements at different time points or frequencies. Anatomical MRI data: High-resolution structural images of the subject's brain, used for source localization and spatial alignment. The FIF file format ensures compatibility and interoperability between different MEG and EEG software tools and facilitates data sharing and collaboration in the neuroimaging research community. It's worth noting that MEG and EEG data can be quite complex and multidimensional, and the FIF format provides a structured and standardized way to store and manage this data.","title":"FIF files"},{"location":"04_data/#asc-files","text":"ASC files in the context of eye tracking refer to data files generated by eye tracking systems, which are used to record and analyze eye movement and gaze behavior. These files contain valuable information about a person's visual attention, including where they are looking, how long they are looking at certain areas, and how their gaze moves across a screen or a visual stimulus. ASC files typically store a time-stamped sequence of gaze data points, which include information such as: Timestamps: The exact time at which each gaze data point was recorded. Gaze Coordinates: The x and y coordinates on the screen where the person's gaze is directed. Pupil Diameter: The size of the person's pupil, which can provide insights into changes in visual processing or cognitive load. Fixations: Periods of stable gaze where the person is looking at a specific point without significant movement. Saccades: Rapid eye movements between fixations, indicating shifts in attention. Blinks: Instances when the person's eyes are closed, which can be important for data cleaning and analysis. Researchers and usability professionals use ASC files to study various aspects of human visual attention and cognitive processes. Eye tracking studies can provide insights into user behavior when interacting with websites, advertisements, software interfaces, or other visual stimuli. By analyzing ASC files, researchers can better understand how people engage with visual information, make decisions, and process visual content. Software tools and programming libraries are often used to process and analyze ASC files, extracting meaningful patterns and insights from the raw gaze data. These insights can be used to optimize user interfaces, design effective visual communication, and improve user experiences in various applications.","title":"ASC files"},{"location":"04_data/#dicom-files","text":"DICOM (Digital Imaging and Communications in Medicine) files are a standard format used for storing, transmitting, and managing medical images and related information. They are widely used in the field of radiology and other medical imaging specialties, such as MRI (Magnetic Resonance Imaging), CT (Computed Tomography), ultrasound, and more. DICOM files are designed to ensure interoperability and compatibility among different imaging devices, picture archiving and communication systems (PACS), and other healthcare information systems. These files contain not only the actual image data but also metadata and contextual information, such as patient information, image acquisition parameters, study details, and more. This comprehensive set of information allows medical professionals to accurately interpret and diagnose medical images, track patient history, and collaborate effectively across different healthcare facilities. DICOM files typically have a \".dcm\" file extension and adhere to a specific structure and data format defined by the DICOM standard. This standardization facilitates seamless exchange of medical images and data between different software and hardware platforms, contributing to improved patient care and medical research.","title":"DICOM files"},{"location":"04_data/#nifti-files","text":"NIFTI (Neuroimaging Informatics Technology Initiative) files are a widely used file format in the field of neuroimaging, particularly in the context of functional and structural magnetic resonance imaging (MRI) data. These files store three-dimensional brain images and related metadata, allowing researchers and clinicians to store, share, and analyze neuroimaging data. NIFTI files typically have the \".nii\" or \".nii.gz\" file extensions. The \".nii\" format is uncompressed, while the \".nii.gz\" format is compressed using gzip compression. These files contain information about the image dimensions, voxel sizes, data type (e.g., integer or floating-point values), and other relevant information. NIFTI was developed as an improvement over the earlier Analyze file format. It addresses certain limitations of the Analyze format, such as the ability to handle 3D and 4D data (3D data over time), improved support for different data types, and better compatibility with modern software tools. Researchers and medical professionals use NIFTI files to store various types of neuroimaging data, including structural MRI scans, functional MRI scans, and diffusion tensor imaging (DTI) data (which captures white matter tracts and connectivity patterns within the brain). NIFTI files are supported by many neuroimaging software packages, making them a crucial part of the neuroimaging data analysis workflow. Researchers can use these files for tasks such as preprocessing, visualization, statistical analysis, and creating anatomical atlases.","title":"NIFTI files"},{"location":"04_data/#npy-files","text":"NPY files, short for \"NumPy files,\" are a file format used in the Python programming language for storing and exchanging numerical data. NumPy is a popular library in Python that provides support for multi-dimensional arrays and matrices, along with various mathematical functions to operate on these arrays. NPY files are specifically used to save and load arrays and data structures created using NumPy. These files have the extension \".npy\" and are binary files that efficiently store the data in a format that preserves the array's shape, data type, and other metadata. This makes them suitable for fast and efficient storage and retrieval of large numerical datasets, which is especially useful in scientific computing, data analysis, and machine learning applications.","title":"NPY files"},{"location":"05_acquisition/","text":"Acquisition Modalities The Cogitate dataset encompasses three distinct neuroimaging modalities, along with synchronized eye-tracking and behavioral data linked to one of these modalities. fMRI data acquisition Anatomical and functional MRI data were acquired on two 3T Prisma scanners (Siemens, Erlangen, Germany), using 32-channel head coils. Anatomical images were acquired using a T1-weighted magnetization prepared rapid gradient echo sequence (MP-RAGE; GRAPPA acceleration factor = 2, TR/TE = 2300/3.03 ms, voxel size 1 mm isotropic, 8\u00b0 flip angle). Functional images were acquired using a whole-brain T2-weighted multiband-4 sequence (time repetition [TR] / time echo [TE] = 1500/39.6 ms, 68 slices, voxel size 2 mm isotropic, 75\u00b0 flip angle, A/P phase encoding direction, FOV = 210 mm, BW = 2090 Hz/Px). In addition, a single band reference image was acquired before each run. To allow for signal stabilization, the first three volumes of each run were discarded. Additional scans using the same T2-weighted sequence, but with inverted phase encoding direction (inverted RO/PE polarity), were collected at multiple points throughout the experiments. Data for Experiment 1 was acquired from 120 Subject in 2 locations: Donders Center for Cognitive Neuroimaging (DCCN) Yale Magnetic Resonance Research Center (MRRC) Standard Operating Procedures for both locations can be found in the following links: Technical checklist PRESCREENING Technical Checklist DCCN Technical Checklist MRRC In depth functional description of the experimental apparatus: Functional_Description.pdf Optical_path_PrismaFit_BOLDscreen.pdf Final parameters for acquisition: FMRI_Blumenfeld_Tempelton_Ex1_final_parameters.pdf iEEG data acquisition intracranial electroencephalography (iEEG, N=34). iEEG SOPs: New York University; Harvard University; Wisconsin University - 25 Subjects from each lab - Total: ca. 26 (is this still right?) - Measurements - SOPs - Techniques - etc. Acquisition sites: Harvard University at Boston Children's Hospital Brigham and Women's Hospital New York University Langone (NYU) University of Wisconsin MEEG data acquisition magnetoencephalography (MEEG, N=102), and MEEG SOPs: Peking University; Birmingham University 50 Subjects from each lab Total: 102 measurements SOPs Techniques etc. Acquisition sites: University of Birmingham's Centre for Human Brain Health (CHBH) Peking University (PKU) Eye tracking and Behavioral data acquisition All modalities acquired Eye Tracking and Behavioral data for Experiment 1 using \u2026","title":"Acquisition Modalities"},{"location":"05_acquisition/#acquisition-modalities","text":"The Cogitate dataset encompasses three distinct neuroimaging modalities, along with synchronized eye-tracking and behavioral data linked to one of these modalities.","title":"Acquisition Modalities"},{"location":"05_acquisition/#fmri-data-acquisition","text":"Anatomical and functional MRI data were acquired on two 3T Prisma scanners (Siemens, Erlangen, Germany), using 32-channel head coils. Anatomical images were acquired using a T1-weighted magnetization prepared rapid gradient echo sequence (MP-RAGE; GRAPPA acceleration factor = 2, TR/TE = 2300/3.03 ms, voxel size 1 mm isotropic, 8\u00b0 flip angle). Functional images were acquired using a whole-brain T2-weighted multiband-4 sequence (time repetition [TR] / time echo [TE] = 1500/39.6 ms, 68 slices, voxel size 2 mm isotropic, 75\u00b0 flip angle, A/P phase encoding direction, FOV = 210 mm, BW = 2090 Hz/Px). In addition, a single band reference image was acquired before each run. To allow for signal stabilization, the first three volumes of each run were discarded. Additional scans using the same T2-weighted sequence, but with inverted phase encoding direction (inverted RO/PE polarity), were collected at multiple points throughout the experiments. Data for Experiment 1 was acquired from 120 Subject in 2 locations: Donders Center for Cognitive Neuroimaging (DCCN) Yale Magnetic Resonance Research Center (MRRC) Standard Operating Procedures for both locations can be found in the following links: Technical checklist PRESCREENING Technical Checklist DCCN Technical Checklist MRRC In depth functional description of the experimental apparatus: Functional_Description.pdf Optical_path_PrismaFit_BOLDscreen.pdf Final parameters for acquisition: FMRI_Blumenfeld_Tempelton_Ex1_final_parameters.pdf","title":"fMRI data acquisition"},{"location":"05_acquisition/#ieeg-data-acquisition","text":"intracranial electroencephalography (iEEG, N=34). iEEG SOPs: New York University; Harvard University; Wisconsin University - 25 Subjects from each lab - Total: ca. 26 (is this still right?) - Measurements - SOPs - Techniques - etc. Acquisition sites: Harvard University at Boston Children's Hospital Brigham and Women's Hospital New York University Langone (NYU) University of Wisconsin","title":"iEEG data acquisition"},{"location":"05_acquisition/#meeg-data-acquisition","text":"magnetoencephalography (MEEG, N=102), and MEEG SOPs: Peking University; Birmingham University 50 Subjects from each lab Total: 102 measurements SOPs Techniques etc. Acquisition sites: University of Birmingham's Centre for Human Brain Health (CHBH) Peking University (PKU)","title":"MEEG data acquisition"},{"location":"05_acquisition/#eye-tracking-and-behavioral-data-acquisition","text":"All modalities acquired Eye Tracking and Behavioral data for Experiment 1 using \u2026","title":"Eye tracking and Behavioral data acquisition"},{"location":"06_data_curation/","text":"Data Curation Procedures This SOP ensures that data goes through a rigorous process of quality control and curation before being released to the public or other authorized users. It also emphasizes the importance of protecting patient privacy by removing or anonymizing sensitive information and follows best practices for data management and release in research or clinical settings. All code for data curation can be found at the following link: https://github.com/Cogitate-consortium/cogcurate [private atm, to be made public] SOP (Standard Operating Procedure) for data release: Step 1: Data Upload and QC (Quality Control) Data is initially uploaded to the xnat-prod (XNAT Production) server. This server is the first point of entry for all data before it is made publicly available. The uploaded data goes through a quality control (QC) process to ensure that it meets the necessary standards and criteria for release. This QC step helps identify any potential issues or errors in the data. Once the data passes QC, it is then shared into Phase 2 by the Data Management Team (DMT). Step 2: Data Transfer to Curate To begin the curation process, data needs to be moved from xnat-prod to a dedicated curation environment called xnat-curate. Setting up xsync (data synchronization) between xnat-prod and xnat-curate ensures that data can be transferred securely and efficiently. xsync helps keep the two environments synchronized and up-to-date. Step 3: Curation Process This step involves the actual curation process, where data is carefully reviewed and prepared for public release. Curation is performed on both metadata and data: Metadata Curation : Ensures that personally identifiable information (PHI) and sensitive data are properly anonymized or removed to protect patient privacy. Validates and checks Clinical Research Forms (CRFs) and other metadata for accuracy and completeness. Reviews and curates any additional experimental or questionnaire (ExQus) data. Data Curation : Data is checked for quality and completeness, and any discrepancies are addressed (usually done during the QC step by the DMT). Image data undergoes defacing to remove facial features, which helps protect patient privacy. Headers and metadata within data files are reviewed and standardized. Quality checks such as FreeSurfer (fs_recon checks) are performed on structural MRI data to ensure accuracy. Step 4: Data Transfer to xnat-public Once the curation process is complete, the curated data is moved from xnat-curate to the final release location, which is typically xnat-public. xnat-public is where the cleaned and curated data is made available to authorized users or the public, depending on the data release policies and permissions. Data in xnat-public should be well-organized and easily accessible to users who have been granted access. TODO: add not explaining that organization in xnat mirrors the bundles","title":"Data Curation Procedures"},{"location":"06_data_curation/#data-curation-procedures","text":"This SOP ensures that data goes through a rigorous process of quality control and curation before being released to the public or other authorized users. It also emphasizes the importance of protecting patient privacy by removing or anonymizing sensitive information and follows best practices for data management and release in research or clinical settings. All code for data curation can be found at the following link: https://github.com/Cogitate-consortium/cogcurate [private atm, to be made public] SOP (Standard Operating Procedure) for data release: Step 1: Data Upload and QC (Quality Control) Data is initially uploaded to the xnat-prod (XNAT Production) server. This server is the first point of entry for all data before it is made publicly available. The uploaded data goes through a quality control (QC) process to ensure that it meets the necessary standards and criteria for release. This QC step helps identify any potential issues or errors in the data. Once the data passes QC, it is then shared into Phase 2 by the Data Management Team (DMT). Step 2: Data Transfer to Curate To begin the curation process, data needs to be moved from xnat-prod to a dedicated curation environment called xnat-curate. Setting up xsync (data synchronization) between xnat-prod and xnat-curate ensures that data can be transferred securely and efficiently. xsync helps keep the two environments synchronized and up-to-date. Step 3: Curation Process This step involves the actual curation process, where data is carefully reviewed and prepared for public release. Curation is performed on both metadata and data: Metadata Curation : Ensures that personally identifiable information (PHI) and sensitive data are properly anonymized or removed to protect patient privacy. Validates and checks Clinical Research Forms (CRFs) and other metadata for accuracy and completeness. Reviews and curates any additional experimental or questionnaire (ExQus) data. Data Curation : Data is checked for quality and completeness, and any discrepancies are addressed (usually done during the QC step by the DMT). Image data undergoes defacing to remove facial features, which helps protect patient privacy. Headers and metadata within data files are reviewed and standardized. Quality checks such as FreeSurfer (fs_recon checks) are performed on structural MRI data to ensure accuracy. Step 4: Data Transfer to xnat-public Once the curation process is complete, the curated data is moved from xnat-curate to the final release location, which is typically xnat-public. xnat-public is where the cleaned and curated data is made available to authorized users or the public, depending on the data release policies and permissions. Data in xnat-public should be well-organized and easily accessible to users who have been granted access. TODO: add not explaining that organization in xnat mirrors the bundles","title":"Data Curation Procedures"},{"location":"07_access/","text":"Access COGITATE data To facilitate access to our data, we offer two main avenues: 1. Archival (bundles) Format : This approach involves providing a collection of links to bundles of data and accompanying metadata. These links grant users the ability to download specific modalities, example datasets, or the complete dataset. Access to these resources requires a registration process on our website: [LINK] . TODO: Add bundles description from bundles.doc 2. \"Live\" Database Release : Our \"live\" release involves an XNAT database instance that contains our data. This database offers a web interface for navigating the data and an API for programmatically retrieving specific datasets based on user interests. Comprehensive instructions on how to register, access, and query our database are provided in the subsequent sections. By disseminating our data through these two avenues, we endeavor to make it accessible and usable for a wide range of scientific investigations. \u2014 Introduce sample data : used as ground truth for analyses pipelines useful to get familiarized with the data structure and contents Explain that users need to register in both of these \"avenues\" bc we want to keep track of who is using and also user agreement stuff please be nice\u2026. Add detailed organization of bundles and xnat projects Link to XNAT DOCS","title":"Access COGITATE data"},{"location":"07_access/#access-cogitate-data","text":"To facilitate access to our data, we offer two main avenues:","title":"Access COGITATE data"},{"location":"07_access/#1-archival-bundles-format","text":"This approach involves providing a collection of links to bundles of data and accompanying metadata. These links grant users the ability to download specific modalities, example datasets, or the complete dataset. Access to these resources requires a registration process on our website: [LINK] . TODO: Add bundles description from bundles.doc","title":"1. Archival (bundles) Format:"},{"location":"07_access/#2-live-database-release","text":"Our \"live\" release involves an XNAT database instance that contains our data. This database offers a web interface for navigating the data and an API for programmatically retrieving specific datasets based on user interests. Comprehensive instructions on how to register, access, and query our database are provided in the subsequent sections. By disseminating our data through these two avenues, we endeavor to make it accessible and usable for a wide range of scientific investigations. \u2014 Introduce sample data : used as ground truth for analyses pipelines useful to get familiarized with the data structure and contents Explain that users need to register in both of these \"avenues\" bc we want to keep track of who is using and also user agreement stuff please be nice\u2026. Add detailed organization of bundles and xnat projects Link to XNAT DOCS","title":"2. \"Live\" Database Release:"},{"location":"08_tutorials/","text":"Tutorials and Examples","title":"Tutorials and Examples"},{"location":"08_tutorials/#tutorials-and-examples","text":"","title":"Tutorials and Examples"},{"location":"09_next/","text":"Next steps Experiment 2 Preprocessed and fully processed data (Bids derivatives)","title":"Next steps"},{"location":"09_next/#next-steps","text":"Experiment 2 Preprocessed and fully processed data (Bids derivatives)","title":"Next steps"},{"location":"10_support/","text":"Support Where to contact us Where to put issues Where to get help","title":"Support"},{"location":"10_support/#support","text":"Where to contact us Where to put issues Where to get help","title":"Support"},{"location":"11_appendix/","text":"Appendix","title":"Appendix"},{"location":"11_appendix/#appendix","text":"","title":"Appendix"}]}